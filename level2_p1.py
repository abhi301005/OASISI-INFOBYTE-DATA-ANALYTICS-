# -*- coding: utf-8 -*-
"""LEVEL2 P1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wtmp-6UMDr2JwEOSstau-L0yVlbduvnD

#  IMPORTING THE LIBRARIES
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pointbiserialr

"""// The first step is to import the librarys for data analysis. numpy is for numerical operations,pandas is for data manipulation and analysis,warning is a built in function it manages warnings while execution,seaborn is for data visualization.

##  MOUNTING THE GOOGLE DRIVE
"""

from google.colab import drive
drive.mount('/content/drive')

"""## IMPORTING THE CSV FILE"""

df= pd.read_csv('/content/drive/My Drive/Housing.csv.xls')

"""// 'df' is a variable,pd is to read the data from the spacified file,read_csv() it is a function from pandas,/content/drive/My Drive/Housing.csv.xls it is the path to access the file."""

df.head()

"""// I used .head() to show the first 5 rows of data present in the file."""

df.tail()

"""// I used .tail() to show the last 5 rows of data present in the file it works same like .head()

## **DESCRIBING THE INFO**
"""

df.describe()

"""// .describe is used to COUNT the data values inculuding the mean, median, mode, standard deviation.



"""

df.info()

"""// .info() is used to provide column details and the type of data is used and also the memory storage."""

df.shape

"""// .shape is used for knowing how many rows and columns are present."""

df.columns

"""// .columns is used to know the names of field names or columns names."""

df.isnull().sum()

"""// .isnull() is check wether there are any null values present in the file if we use .sum() we can see the how many values are null.

## **CHECKING THE UNIQUE VALUES**
"""

df.nunique()

"""//.nunique is used for find how many unique values are present inside the every column.


"""

fig, axs = plt.subplots(2,3, figsize = (10,5))
plt1 = sns.boxplot(df['price'], ax = axs[0,0])
plt2 = sns.boxplot(df['area'], ax = axs[0,1])
plt3 = sns.boxplot(df['bedrooms'], ax = axs[0,2])
plt1 = sns.boxplot(df['bathrooms'], ax = axs[1,0])
plt2 = sns.boxplot(df['stories'], ax = axs[1,1])
plt3 = sns.boxplot(df['parking'], ax = axs[1,2])
plt.tight_layout()

"""//  Price and area have considerable outliers. We can drop the outliers as we have sufficient data."""

plt.boxplot(df.price)
Q1 = df.price.quantile(0.25)
Q3 = df.price.quantile(0.75)
IQR = Q3 - Q1
housing = df[(df.price >= Q1 - 1.5*IQR) & (df.price <= Q3 + 1.5*IQR)]

plt.boxplot(df.area)
Q1 = df.area.quantile(0.25)
Q3 = df.area.quantile(0.75)
IQR = Q3 - Q1
df = df[(df.area >= Q1 - 1.5*IQR) & (df.area <= Q3 + 1.5*IQR)]

fig, axs = plt.subplots(2,3, figsize = (10,5))
plt1 = sns.boxplot(df['price'], ax = axs[0,0])
plt2 = sns.boxplot(df['area'], ax = axs[0,1])
plt3 = sns.boxplot(df['bedrooms'], ax = axs[0,2])
plt1 = sns.boxplot(df['bathrooms'], ax = axs[1,0])
plt2 = sns.boxplot(df['stories'], ax = axs[1,1])
plt3 = sns.boxplot(df['parking'], ax = axs[1,2])

plt.tight_layout()

"""# **EXPLORATORY DATA ANALYTICS**"""

sns.pairplot(df)
plt.show()

"""//  Visualising Numeric Variables."""

plt.figure(figsize=(20, 12))

# Define a custom palette for 'yes' and 'no'
custom_palette = {'yes': 'skyblue', 'no': 'salmon'}

plt.subplot(2,3,1)
sns.boxplot(x = 'mainroad', y = 'price', data = df, palette=custom_palette)
plt.subplot(2,3,2)
sns.boxplot(x = 'guestroom', y = 'price', data = df, palette=custom_palette)
plt.subplot(2,3,3)
sns.boxplot(x = 'basement', y = 'price', data = df, palette=custom_palette)
plt.subplot(2,3,4)
sns.boxplot(x = 'hotwaterheating', y = 'price', data = df, palette=custom_palette)
plt.subplot(2,3,5)
sns.boxplot(x = 'airconditioning', y = 'price', data = df, palette=custom_palette)
plt.subplot(2,3,6)
sns.boxplot(x = 'furnishingstatus', y = 'price', data = df, palette='rocket') # Keep original palette for this one
plt.show()

"""//  Visualising Categorical Variables.

//  We can also visualise some of these categorical features parallely by using the hue argument. Below is the plot for furnishingstatus with airconditioning as the hue.
"""

plt.figure(figsize = (10, 5))
sns.boxplot(x = 'furnishingstatus', y = 'price', hue = 'airconditioning', data = df)
plt.show()

"""## **DATA PREPARATION**"""

varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']

def binary_map(x):
    return x.map({'yes': 1, "no": 0})

housing[varlist] = housing[varlist].apply(binary_map)
display(housing.head())

"""# **DUMMY VARIABLES**"""

status = pd.get_dummies(housing['furnishingstatus'])

status.head()

"""//  In this code snippet retrieves and displays the rows with index labels 15 through 19 from your status DataFrame, allowing you to inspect the one-hot encoded values for these specific data points."""

status = pd.get_dummies(df['furnishingstatus'], drop_first = True)

df= df.drop('furnishingstatus', axis=1)
status = status.astype(int)
df = pd.concat([housing, status], axis = 1)
df.head()

"""//  The variable furnishingstatus has three levels. We need to convert these levels into integer as well.For this, we will use something called dummy variables.

## **SPLITTING THE DATA INTO TRAINING AND TESTING SETS**
"""

from sklearn.model_selection import train_test_split
import numpy as np
housing.dropna(inplace=True)
np.random.seed(0)
df_train, df_test = train_test_split(housing, train_size = 0.7, test_size = 0.3, random_state = 100)

"""//. A crucial data preparation step that cleans your main dataset by removing missing values and then divides it into separate sets for training your model (df_train) and evaluating its performance (df_test), ensuring reproducibility of the split

## **RESCALING THE FEATURES USING MINMAX SCALING**
"""

from sklearn.preprocessing import MinMaxScaler

"""//. The MinMaxScaler is a data scaling technique. Scaling, also known as feature scaling or standardization, is a preprocessing step used to normalize the range of independent variables or features of data. In this case, MinMaxScaler scales features to a given range, usually between 0 and 1. This is particularly useful for algorithms that are sensitive to the magnitude of features, such as gradient descent-based algorithms or distance-based algorithms like K-Nearest Neighbors."""

scaler = MinMaxScaler()

num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']

df_train[num_vars] = scaler.fit_transform(df_train[num_vars])

display(df_train.head())

"""//   The purpose of displaying the head of df_train at this point is to allow you to quickly inspect the data after the MinMaxScaler has been applied in the preceding cell."""

df_train.describe()

"""//  The purpose of displaying the descriptive statistics at this point is to allow you to examine the distribution and summary of your data after scaling."""

plt.figure(figsize = (16, 10))
sns.heatmap(df_train.drop('furnishingstatus', axis=1).corr(), annot = True, cmap="rocket")
plt.show()

"""## **DIVIDING INTO X AND Y SETS FOR THE MODEL BUILDING**"""

y_train = df_train.pop('price')
X_train = df_train

"""## **MODEL BUILDING**"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

lm = LinearRegression()
lm.fit(X_train.dropna().drop('furnishingstatus', axis=1), y_train.dropna())

rfe = RFE(estimator=lm, n_features_to_select=6)
rfe = rfe.fit(X_train.drop('furnishingstatus', axis=1), y_train)

display(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))

col = X_train.drop('furnishingstatus', axis=1).columns[rfe.support_]
col

X_train.drop('furnishingstatus', axis=1).columns[~rfe.support_]

"""## **BUILDING MODEL**"""

X_train_rfe = X_train[col]

import statsmodels.api as sm
X_train_rfe = sm.add_constant(X_train_rfe)

X_train_rfe = X_train_rfe.astype(float)
lm = sm.OLS(y_train,X_train_rfe).fit()

lm.summary()

from statsmodels.stats.outliers_influence import variance_inflation_factor

vi = pd.DataFrame()
X = X_train_rfe
vi['Features'] = X.columns
vi['VI'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vi['VI'] = round(vi['VI'], 2)
vi = vi.sort_values(by = "VI", ascending = False)
vi

"""## **RESIDUAL ANALYSIS OF THE TRAIN DATA**"""

y_train_price = lm.predict(X_train_rfe)

res = (y_train_price - y_train)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

fig = plt.figure()
sns.distplot((y_train - y_train_price), bins = 20)
fig.suptitle('Error Terms', fontsize = 20)
plt.xlabel('Errors', fontsize = 18)

plt.scatter(y_train,res)
plt.show()

"""//  There may be some relation in the error terms.

## **MODEL EVALUATION**
"""

num_vars = ['area','stories', 'bathrooms', 'airconditioning', 'prefarea','parking','price']

df_test[num_vars] = scaler.fit_transform(df_test[num_vars])

"""## **DIVIDING INTO X_TEST AND Y_TEST**"""

y_test = df_test.pop('price')
X_test = df_test

X_test = sm.add_constant(X_test)

X_test_rfe = X_test[X_train_rfe.columns]

y_pred = lm.predict(X_test_rfe)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

print("Columns in X_train_rfe:")
display(X_train_rfe.columns)
print("\nColumns in X_test_rfe:")
display(X_test_rfe.columns)

fig = plt.figure()
plt.scatter(y_test,y_pred)
fig.suptitle('y_test vs y_pred', fontsize=20)
plt.xlabel('y_test', fontsize=18)
plt.ylabel('y_pred', fontsize=16)

"""//  If the model's predictions were perfect, all the points would lie exactly on a diagonal line (where y_test equals y_pred).
Points that are close to this diagonal line indicate good predictions.
Points that are far away from the diagonal line represent predictions where the model significantly overestimated or underestimated the actual price.

"""