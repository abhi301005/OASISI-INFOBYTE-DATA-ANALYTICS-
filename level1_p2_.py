# -*- coding: utf-8 -*-
"""LEVEL1 P2 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112usJQsexsQiVHUOw7vON9AQiJ6oQAJS

# **1. IMPORTING THE LIBRARIES**
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pointbiserialr

"""// The first step is to import the librarys for data analysis. numpy is for numerical operations,pandas is for data manipulation and analysis,warning is a built in function it manages warnings while execution,seaborn is for data visualization.

# **2. MOUNTING THE GOOGLE DRIVE**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **3. IMPORTING THE CSV FILE**"""

df = pd.read_csv('/content/drive/My Drive/ifood_df.csv')

"""// 'df' is a variable,pd is to read the data from the spacified file,read_csv() it is a function from pandas,/content/drive/My Drive/ifood_df.csv. it is the path to access the file.

## **4.DESCRIBING THE INFO**
"""

df.describe()

"""// .describe is used to COUNT the data values inculuding the mean, median, mode, standard deviation."""

df.head()

"""// I used .head() to show the first 5 rows of data present in the file."""

df.tail()

"""// I used .tail() to show the last 5 rows of data present in the file it works same like .head()."""

df.info()

"""// .info() is used to provide column details and the type of data is used and also the memory storage."""

df.shape

"""// .shape is used for knowing how many rows and columns are present."""

df.corr()

"""// This is used to know he pairwise correlation of columns in the DataFrame"""

df.columns

"""// .columns is used to know the names of field names or columns names.

## **5.CHECKING THE DATA WHETHER THERE ARE ANY NULL VALUES IN THE FILE**
"""

df.isnull().sum()

"""// .isnull() is check wether there are any null values present in the file if we use .sum() we can see the how many values are null.

## **6.CHECKING THE UNIQUE VALUES**
"""

df.nunique()

"""//.nunique is used for find how many unique values are present inside the every column.

## **7.VISUALIZING THE DATA WITH BOX PLOT HISTOGRAM, PIE CHART, SCATTER PLOTS, CORRELATION MATRIX , BINARY VALUES CORRELATIONS SHARE.**
"""

Q1 = df['MntTotal'].quantile(0.25)
Q2 = df['MntTotal'].median()
Q3 = df['MntTotal'].quantile(0.75)
IQR = Q3 - Q1
lower_whisker = max(df['MntTotal'].min(), Q1 - 1.5 * IQR)
upper_whisker = min(df['MntTotal'].max(), Q3 + 1.5 * IQR)
outliers = df[(df['MntTotal'] < lower_whisker) | (df['MntTotal'] > upper_whisker)]['MntTotal']
plt.figure(figsize=(8, 5))
sns.boxplot(data=df, y='MntTotal', color='skyblue', showmeans=True,
            meanprops={"marker": "o", "markerfacecolor": "red",
                       "markeredgecolor": "black", "markersize":"8"})
plt.axhline(Q1, color='green', linestyle='--', label=f'Q1: {Q1:.2f}')
plt.axhline(Q2, color='blue', linestyle='-' , label=f'Median (Q2): {Q2:.2f}')
plt.axhline(Q3, color='green', linestyle='--', label=f'Q3: {Q3:.2f}')
plt.axhline(lower_whisker, color='purple' , linestyle=':' , label=f'Lower Whisker: {lower_whisker:.2f}')
plt.axhline(upper_whisker, color='purple', linestyle=':', label=f'Upper Whisker: {upper_whisker:.2f}')
plt.text(0.05, upper_whisker, f'Outliers: {len(outliers)}',
         fontsize=10, color='red', verticalalignment='bottom')
plt.title('Enhanced Box Plot for MntTotal')
plt.ylabel('MntTotal')
plt.legend (loc='upper right')
plt.show()

"""//.  Median (Q2) - The middle value of the dataset.
First Quartile (Q1, 25th percentile) - The median of the lower half of the data.
Third Quartile (Q3, 75th percentile) - The median of the upper half of the data.
Interquartile Range (IQR) - The range between Q1 and Q3 (Q3 - Q1).
Whiskers - Extend to the smallest and largest values within 1.5 * IQR
Outliers - Points outside the whiskers, shown as individual dots.

# **REMOVING THE OUTLINERS**
"""

df = df[(df['MntTotal'] > lower_bound) & (df['MntTotal'] < upper_bound)]
df.describe()

"""// First line filters the DataFrame. It keeps only the rows where the value in the 'MntTotal'.this code is used to removes the extreme values in the 'MntTotal' column and then shows you a summary of the data without those outliers."""

Q1 = df['MntTotal'].quantile(0.25)
Q3 = df['MntTotal'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['MntTotal'] < lower_bound) | (df['MntTotal'] > upper_bound)]
outliers.head()

"""// This code is used to identifies extreme values in the 'MntTotal' column that fall outside the typical range defined by the IQR method and shows you a preview of those outlier data points.

## **Boxplot for Income**
"""

plt.figure(figsize=(6, 4))
sns.boxplot(data=df, y='Income', palette='viridis')
plt.title('Box Plot for Income')
plt.ylabel('Income')
plt.show()

"""//

# **Histogram for Income**
"""

plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Income', bins=30, kde=True)
plt.title('Histogram for Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

"""## **Histogram for Age**"""

plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Age', bins=30, kde=True)
plt.title('Histogram for Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

"""## **Skewness**
Skewness measures the asymmetry of a data distribution. It tells us whether the data is skewed to the left (negative skew) or the right (positive skew).
Positive skew (Right-skewed):
Tail is longer on the right side; mean > median > mode.
Negative skew (Left-skewed): Tail is longer on the left side; mean ‹ median < mode.
Zero skew (Symmetric distribution):
 Mean = median = mode, like in a normal distribution.
Kurtosis
Kurtosis measures the tailedness of a distribution-how heavy or light the tails are compared to a normal distribution.
Leptokurtic (Kurtosis > 3):
Heavy tails, more outliers (e.g., financial returns).
Mesokurtic (Kurtosis = 3):
 Normal distribution.
Platykurtic (Kurtosis < 3):
 Light tails, fewer extreme values.

# **Kurtosis**
Kurtosis measures the tailedness of a distribution-how heavy or light the tails are compared to a normal distribution.
Leptokurtic (Kurtosis « 3): Heavy tails, more outliers (e.., financial returns).
Mesokurtic (Kurtosis = 3): Normal distribution.
Platykurtic (Kurtosis < 3): Light tails, fewer extreme values.
"""

print(f"Skewness: {df['Age'].skew():.6f}\nKurtosis: {df['Age'].kurt():.6f}")

"""## **Correlation matrix**

There are many columns in the data. The correlation matrix will be very crowded if we use all columns of the data frame. We will group the columns and explore correlation between columns in each group and the column 'MntTotal'. We will focus on the column 'MntTotal' to understand how we can segment the customers who buy the most in overall. We can run similar analysis for every type of product.
"""

cols_demographics = ['Income','Age']
cols_children = ['Kidhome', 'Teenhome']
cols_marital = ['marital_Divorced', 'marital_Married','marital_Single', 'marital_Together', 'marital_Widow']
cols_mnt = ['MntTotal', 'MntRegularProds','MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
cols_communication = ['Complain', 'Response', 'Customer_Days']
cols_campaigns = ['AcceptedCmpOverall', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']
cols_source_of_purchase = ['NumDealsPurchases', 'NumWebPurchases','NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']
cols_education = ['education_2n Cycle', 'education_Basic', 'education_Graduation', 'education_Master', 'education_PhD']
corr_matrix = df[['MntTotal']+cols_demographics+cols_children].corr()
plt.figure(figsize=(6,6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""## **Point-Biserial correlations for binary variables**
Pearson correlation measures the strength and direction of a linear relationship between two continuous variables.
We used Pearson correlation for MntTotal, Age and Income. When we try to understand the relationship between a continuous variable MntTotal and binary variables like marital status then we should use Point-Biserial Correlation Point-Biserial Correlation is used to measure the strength and direction of the linear relationship between a binary variable and a continuous variable.
"""

for col in cols_marital:
    correlation, p_value = pointbiserialr(df[col], df['MntTotal'])
    print(f'{correlation:.4f}: Point-Biserial Correlation for {col} with p-value {p_value:.4f}')

for col in cols_education:
    correlation, p_value = pointbiserialr(df[col], df['MntTotal'])
    print(f'{correlation:.4f}: Point-Biserial Correlation for {col} with p-value {p_value:.4f}')

"""// There is no strong Point-Biserial correlation between MntTotal and different marital statuses. Some feature engineering may be required during the modelling process.

## **Feature Engineering**

New feature: Marital New feature:In_relationship New feature: Marital The data frame contains 5 columns to reflect marital status. We are going to create a new column 'marital' with values: Divorced, Married, Single, Together, Widow. This column will allow us to draw some additional plots.
"""

def get_marital_status(row):
    if row['marital_Divorced'] == 1:
        return 'Divorced'
    elif row['marital_Married'] == 1:
        return 'Married'
    elif row['marital_Single'] == 1:
        return 'Single'
    elif row['marital_Together'] == 1:
        return 'Together'
    elif row['marital_Widow'] == 1:
        return 'Widow'
    else:
        return 'Unknown'

df['Marital'] = df.apply(get_marital_status, axis=1)
plt.figure(figsize=(8, 6))
sns.barplot(x='Marital', y='MntTotal', data=df, palette='viridis')
plt.title('MntTotal by marital status')
plt.xlabel('Marital status')
plt.ylabel('MntTotal')

"""//  This bar grph represents the Marital status"""

def get_relationship(row):
    if row['marital_Married'] == 1:
        return 1
    elif row['marital_Together'] == 1:
        return 1
    else:
        return 0

df['In_relationship'] = df.apply(get_relationship, axis=1)
df.head()

"""## **K-Means Clustering**
This is a unsupervised machine learning algorithm used to cluster data based on similarity. K-means clustering usually works well in practice and scales well to the large datasets.
In this section:
Standardising data Principal Component Analysis (PCA) Elbow method Silhouette score analysis
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
cols_for_clustering = ['Income', 'MntTotal', 'In_relationship']
data_scaled = df.copy()
data_scaled[cols_for_clustering] = scaler.fit_transform(df[cols_for_clustering])
data_scaled[cols_for_clustering].describe()

"""## **Elbow method**
This method is a technique used to determine the optimal number of clusters (K) for K-means clustering algorithm.
"""

from sklearn import decomposition
pca = decomposition.PCA(n_components = 2)
pca_res = pca.fit_transform(data_scaled[cols_for_clustering])
data_scaled['pc1'] = pca_res[:,0]
data_scaled['pc2'] = pca_res[:,1]
X = data_scaled[cols_for_clustering]
inertia_list = []
for K in range(2,10):
    inertia = KMeans(n_clusters=K, random_state=7).fit(X).inertia_
    inertia_list.append(inertia)

plt.figure(figsize=[7,5])
plt.plot(range(2,10), inertia_list, color=(54 / 255, 113 / 255, 130 / 255))
plt.title("Inertia vs. Number of Clusters")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.show()

"""## **Silhouette score analysis**
Silhouette score is a metric that used to assess the quality of clustering.
"""

from sklearn.metrics import silhouette_score
silhouette_list = []
for K in range(2,10):
    model = KMeans(n_clusters = K, random_state=7)
    clusters = model.fit_predict(X)
    s_avg = silhouette_score(X, clusters)
    silhouette_list.append(s_avg)

plt.figure(figsize=[7,5])
plt.plot(range(2,10), silhouette_list, color=(54 / 255, 113 / 255, 130 / 255))
plt.title("Silhouette Score vs. Number of Clusters")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Silhouette Score")
plt.show()

model = KMeans(n_clusters=4, random_state = 7)
model.fit(data_scaled[cols_for_clustering])
data_scaled['Cluster'] = model.predict(data_scaled[cols_for_clustering])

plt.figure(figsize=(8, 6))
sns.scatterplot(x='pc1', y='pc2', data=data_scaled, hue='Cluster', palette='viridis')
plt.title('Clustered Data Visualization')
plt.xlabel('Principal Component 1 (pc1)')
plt.ylabel('Principal Component 2 (pc2)')
plt.legend(title='Clusters')

df['Cluster'] = data_scaled.Cluster
df.groupby('Cluster')[cols_for_clustering].mean()

"""Mean consumption of different product types by cluster"""

mnt_data = df.groupby('Cluster')[cols_mnt].mean().reset_index()
mnt_data.head()

melted_data = pd.melt(mnt_data, id_vars="Cluster", var_name="Product", value_name="Consumption")
plt.figure(figsize=(12, 6))
sns.barplot(x="Cluster", y="Consumption", hue="Product", data=melted_data, ci=None, palette="viridis")
plt.title("Product Consumption by Cluster")
plt.xlabel("Cluster")
plt.ylabel("Product Consumption")
plt.xticks(rotation=0)
plt.legend(title="Product", loc="upper right")
plt.show()

"""// The above bar graph represnts the Product Consumption by Cluster."""

cluster_sizes = df.groupby('Cluster')[['MntTotal']].count().reset_index()
plt.figure(figsize=(8,6))
sns.barplot(x='Cluster', y='MntTotal', data=cluster_sizes, palette = 'viridis')
plt.title('Cluster sizes')
plt.xlabel('Cluster')
plt.ylabel('MntTotal')

total_rows = len(df)
cluster_sizes['Share%'] = round(cluster_sizes['MntTotal'] / total_rows*100,0)
cluster_sizes.head()

"""## **BOX PLOT**"""

plt.figure(figsize=(8, 6))
sns.boxplot(x='Cluster', y='Income', data=df, palette='viridis')
plt.title('Income by cluster')
plt.xlabel('Cluster')
plt.ylabel('Income')
plt.legend(title='Clusters')

"""// THE ABOVE FIG IS USED TO EXPLAIN ABOUT THE INCOME BASED ON CLUSTER."""

plt.figure(figsize=(8, 6))
sns.scatterplot(x='Income', y='MntTotal', data=df, hue = 'Cluster', palette= 'coolwarm')
plt.title('Income by cluster')
plt.xlabel('Income')
plt.ylabel( 'MntTotal')
plt.legend(title= 'Clusters')

from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score, mean_squared_error, r2_score, accuracy_score
features = ['Income', 'Kidhome',
'Teenhome', 'Recency', 'MntWines',
'MntFruits', 'MntMeatProducts',
'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'MntTotal', 'MntRegularProds', 'AcceptedCmpOverall']
scaler = StandardScaler ()
df_scaled = scaler. fit_transform(df[features])

neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(df_scaled)
distances,indices = neighbors_fit.kneighbors(df_scaled)
distances = np. sort(distances[:, 4], axis=0)

dbscan = DBSCAN (eps=1.5,
min_samples=5)
df[ 'Cluster'] = dbscan. fit_predict(df_scaled)

"""
//. The Silhouette Score measures how well-separated and compact clusters are in an unsupervised learning model (like K-Means, DBSCAN, or
GMM). It evaluates the cohesion (within-cluster tightness) and separation (between-cluster distance). *italicised text*"""

if len(set (df[ 'Cluster'])) > 1:
  silhouette_avg = silhouette_score(df_scaled[df['Cluster'] != -1], df[df[ 'Cluster'] != -1]['Cluster'])
  print(f'Silhouette Score: {silhouette_avg}')
else:
  print( 'Silhouette Score cannot be computed - only one cluster found.')

"""### TYPES OF ERRORS FOR MODEL PERFORMANCE"""

def calculate_mse(df_scaled, df_original, features):
  mse_values = []
  for cluster in set(df_original['Cluster']):
    if cluster != -1:
      cluster_data_scaled = df_scaled[df_original['Cluster'] == cluster]
      cluster_center_scaled = cluster_data_scaled.mean(axis=0)
      mse = mean_squared_error(cluster_data_scaled, np.full(cluster_data_scaled.shape, cluster_center_scaled))
      mse_values.append(mse)

  return np.mean(mse_values) if mse_values else None

mse_score = calculate_mse(df_scaled, df, features)
print(f'Mean Squared Error (MSE): {mse_score}')

"""// The above code is used to find MSE."""

def calculate_rmse(df_scaled, df_original):
  rmse_values = []
  for cluster in set(df_original['Cluster']):
    if cluster != -1:
      cluster_data_scaled = df_scaled[df_original['Cluster'] == cluster]
      cluster_center_scaled = cluster_data_scaled.mean(axis=0)
      mse = mean_squared_error(cluster_data_scaled, np.full(cluster_data_scaled.shape, cluster_center_scaled))
      rmse = np.sqrt(mse)
      rmse_values.append(rmse)

  return np.mean(rmse_values) if rmse_values else None

"""##K-Distance Plot for DBSCAN

"""

plt.figure(figsize=(10, 5))
plt.plot(distances)
plt.xlabel( 'Data Points')
plt.ylabel('Epsilon (k-distance) ')
plt.title( 'Elbow Method for DBSCAN Epsilon' )
plt.show()

"""//  The idea behind the "Elbow Method" for DBSCAN is to look for a point in this sorted k-distance plot where there is a significant "elbow" or bend."""

features = df[[ 'MntTotal',
'Income', 'Age']]
scaler = StandardScaler()
X_scaled = scaler. fit_transform (features)
wcss = []
K_range = range (1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans. fit(X_scaled)
    wcss. append (kmeans.inertia_)
plt. figure(figsize=(8, 5))
plt. plot(K_range, wcss, marker='o', linestyle='-', color='b')
plt. xlabel( 'Number of Clusters (k)')
plt.ylabel( 'Within-Cluster Sum of Squares (WCSS) ')
plt.title('Elbow Method for Optimal k')
plt.xticks(K_range)
plt.grid (True)
plt. show()

"""//  The Elbow Method plot visually shows how the WCSS decreases as the number of clusters increases.

## Silhouette Scores for DBSCAN (min_samples variation)
"""

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

for min_samples in range (3, 10):
    dbscan = DBSCAN(eps=0.5, min_samples=min_samples)
    labels = dbscan.fit_predict(X)
    score = silhouette_score(X, labels)
    print(f"Silhouette Score for min_samples={min_samples}: {score}")

"""//  This code runs DBSCAN multiple times with a fixed eps but varying min_samples and reports the silhouette score for each run.

## **CHECKING THE PERCENTAGE OF NOISE POLUTION**
"""

labels = dbscan.fit_predict (X)
num_noise = np.sum (labels == -1)
total_points = len(labels)
noise_percentage = (num_noise / total_points) * 100
print(f"Noise Percentage: {noise_percentage:.2f}%")

"""//  This code checks the results of your DBSCAN clustering to see how many data points were not assigned to any cluster (considered noise) and reports this as a percentage of the total data points.

## **PLOTING THE NOISE POLUTION ON POINT GRAPH**
"""

eps_values = np.arange (0.1, 1.0, 0.1)
noise_percentages = []
for eps in eps_values:
      min_samples = int(np.log(len(X)))
      dbscan = DBSCAN(eps=eps, min_samples=min_samples)
      labels = dbscan. fit_predict(X)
      num_noise = np. sum (labels == -1)
      noise_percentages.append ((num_noise / len(labels)) * 100)
plt. figure(figsize=(8, 5))
plt.plot(eps_values,
noise_percentages, marker='o', linestyle='-')
plt. xlabel( 'Epsilon (eps) ')
plt.ylabel( 'Noise Percentage')
plt.title('Effect of eps on Noise Percentage in DBSCAN' )
plt.grid (True)
plt.show()

"""//   This plot helps you visualize how the percentage of noise changes as you increase the eps value, which we can assist on selecting a suitable eps for the DBSCAN model.

## **COMPARING DBSCAN WITH K-Means**
"""

kmeans = KMeans(n_clusters=3,
random_state=42)
kmeans_labels = kmeans. fit_predict (X)
kmeans_score = silhouette_score(X, kmeans_labels)
print(f"K-Means Silhouette Score: {kmeans_score}")

"""//  This code performs K-Means clustering with K=3 and then calculates and displays the silhouette score to give you a quantitative measure of how well the data points are clustered.

# FINDING THE GOOD SILHOUETTE SCORE FOR K-Means
"""

K_range = range (2, 10)
silhouette_scores = []
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans. fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append (score)

plt. figure(figsize=(8, 5))
plt.plot(K_range, silhouette_scores, marker='o' , linestyle='-')
plt. xlabel( 'Number of Clusters (k)')
plt. ylabel( 'Silhouette Score')
plt. title( 'Silhouette Score for Different k')
plt.grid (True)
plt.show()

best_k = K_range[np.argmax(silhouette_scores) ]
print(f"Optimal k based on silhouette score: {best_k}")

"""## **Standardize the Data**"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler ()
X_scaled = scaler. fit_transform(X)

"""## **USEING PCA(Principal Component Analysis) TO REDUCE DIMENSIONS**"""

from sklearn. decomposition import PCA
pca = PCA(n_components=2)

X_pca = pca.fit_transform(X_scaled)
kmeans = KMeans(n_clusters=best_k, random_state=42)
kmeans_labels = kmeans. fit_predict(X_pca)
new_score = silhouette_score(X_pca, kmeans_labels)
print(f"New Silhouette Score after PCA: {new_score}")