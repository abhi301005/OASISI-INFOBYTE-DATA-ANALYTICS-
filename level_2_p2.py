# -*- coding: utf-8 -*-
"""LEVEL 2 P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F6WPck4wtRPTmChzGIB5RP5w7OotRUb5

## **IMPORTING THE LIBRARYS**
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pointbiserialr

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
# %matplotlib inline

"""// The first step is to import the librarys for data analysis. numpy is for numerical operations,pandas is for data manipulation and analysis,warning is a built in function it manages warnings while execution,seaborn is for data visualization.

# **IMPORTING THE DATA FROM GOOGLE BY USING LINK**
"""

import pandas as pd

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')
display(df.head())

"""// 'df' is a variable,pd is from  panadas library and the path is the address of the dataset."""

df.head()

"""// I used .head() to show the first 5 rows of data present in the file."""

df.tail()

"""// I used .tail() to show the last 5 rows of data present in the file it works same like .head()."""

df.columns

"""//. For knowing how any rows and columns."""

df.shape

"""// .shape is used for knowing how many rows and columns are present."""

df.isnull().sum()

"""// .isnull() is check wether there are any null values present in the file if we use .sum() we can see the how many values are null."""

df.nunique()

"""// Checking for any unique values.

## **DISCRIBING THE DATASET**
"""

df.describe()

"""
// .describe is used to COUNT the data values inculuding the mean, median, mode, standard deviation."""

df.info()

"""// .info() is used to provide column details and the type of data is used and also the memory storage."""

df.corr()

"""The output is a table where each row and column represents a column in the dataset DataFrame, and the value at the intersection of a row and column is the correlation coefficient between those two columns.

## **CHECKING EVERY ITEMS PRESENT IN THE WINE AND THERE QUALITY.**
"""

import matplotlib.pyplot as plt
fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'fixed acidity', data = df)

"""// Checking the quantity and quality and the fixed acidity with the help of bargraph."""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'volatile acidity', data = df)

"""//.  checking the quality and the volatile acidity quantity in wine with the help of bargraph."""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'citric acid', data = df)

"""//. In this bargraph we are checking quality and the citric acid quantity present in wine."""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'residual sugar', data = df)

"""//  In this bargraph represent quality and residual sugar present in the wine."""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'chlorides', data = df)

"""//. Checking the chlorides present in wine."""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'free sulfur dioxide', data = df)

"""//. Free sulfur dioxide with the quality"""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'total sulfur dioxide', data = df)

"""//. Total sulfur dioxide with the quality"""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'sulphates', data = df)

"""//. Sulphates with the quality"""

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'alcohol', data = df)

"""//. Alchol with the quality

## **PREPROCESSING DATA FOR PERFORMING MACHINE LEARNING ALGORITHMS**
"""

bins=(2,6.5,8)
group_names = ['bad', 'good']
df['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)

label_quality = LabelEncoder()

df['quality'] = label_quality.fit_transform(df['quality'])

df['quality'].value_counts()

X = df.drop('quality', axis = 1)
y = df['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

"""## **RANDOM FOREST CLASSIFIER**"""

rfc = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)
rfc.fit(X_train, y_train)
pred_rfc = rfc.predict(X_test)
y_rfc = rfc.predict(X_test)
accuracy_rfc = accuracy_score(y_test, y_rfc)

print(classification_report(y_test, pred_rfc))

"""# **RANDOM FOREST GIVES THE ACCURACY OF 89%**"""

print(confusion_matrix(y_test, pred_rfc))

"""## **STOCHASTIC GRADIENT DESCENT CLASSIFIER**"""

print(y_train.value_counts())  # For pandas Series
# or
import numpy as np
print(np.unique(y_train, return_counts=True))  # For NumPy array

sgd = SGDClassifier(loss='modified_huber', shuffle=True, random_state=42, class_weight='balanced')
sgd.fit(X_train, y_train)
pred_sgd = sgd.predict(X_test)
print(classification_report(y_test, pred_sgd))

print(confusion_matrix(y_test, pred_sgd))

"""## **SUPPORT VECTOR CLASSIFIER**"""

svc = SVC()
svc.fit(X_train, y_train)
pred_svc = svc.predict(X_test)
y_svc = svc.predict(X_test)
accuracy_svc = accuracy_score(y_test, y_svc)

print(classification_report(y_test, pred_svc))

"""//  Support vector classifier gives 87% accuracy

## GRID SEARCH
"""

param = {
    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],
    'kernel':['linear', 'rbf'],
    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]
}
grid_svc = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=10)

grid_svc.fit(X_train, y_train)

grid_svc.best_params_

svc2 = SVC(C = 1.2, gamma =  0.8, kernel= 'rbf')
svc2.fit(X_train, y_train)
pred_svc2 = svc2.predict(X_test)
print(classification_report(y_test, pred_svc2))

"""//  SVC improves from 87% to 94% using Grid Search CV.

## **CROSS VALIDATION SCORE FOR RANDOM FOREST AND SGD**
"""

rfc_eval = cross_val_score(estimator = rfc, X = X_train, y = y_train, cv = 10)
rfc_eval.mean()

"""// Random forest accuracy increases from 89% to 90% using cross validation score

## **ANALYZING CHEMICAL QUALITIES**
"""

features = ["fixed acidity","volatile acidity","citric acid","chlorides","total sulfur dioxide","density","sulphates","alcohol","residual sugar","free sulfur dioxide","pH"]
quality_potential = 'quality'
for feature in features:
    plt.figure(figsize=(3, 3))
    plt.scatter(df[feature], df[quality_potential], alpha=0.5)
    plt.title(f'{feature} vs quality_potential')
    plt.xlabel(feature)
    plt.ylabel('quality_potential')
    plt.grid(True)
    plt.show()

import seaborn as sns
corr_mat = df.corr()
plt.figure(figsize=(20, 20))
sns.heatmap(corr_mat, annot=True, cmap='rocket', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""//  From the above heatmap we can conclude that the 'total sulphur dioxide' and 'free sulphur dioxide' are highly correlated."""

def display_boxplots(dataset):
    for column in dataset.columns:
        plt.figure()
        plt.boxplot(dataset[column])
        plt.title(f'Boxplot of {column}')
        plt.show()
def drop_outliers_iqr(dataset, threshold=1):
    filtered_df = pd.DataFrame()
    for column in dataset.columns:
        Q1 = dataset[column].quantile(0.25)
        Q3 = dataset[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        filtered_df[column] = dataset[(dataset[column] >= lower_bound) & (dataset[column] <= upper_bound)][column]
    return filtered_df
filtered_df = drop_outliers_iqr(df)
filtered_df = filtered_df.dropna()
display_boxplots(filtered_df)

features = ["fixed acidity","volatile acidity","citric acid","chlorides","total sulfur dioxide","density","sulphates","alcohol","residual sugar","free sulfur dioxide","pH"]
quality_potential = 'quality'
for feature in features:
    plt.figure(figsize=(3, 3))
    plt.scatter(filtered_df[feature], filtered_df[quality_potential], alpha=0.5)
    plt.title(f'{feature} vs quality_potential')
    plt.xlabel(feature)
    plt.ylabel('quality_potential')
    plt.grid(True)
    plt.show()

"""## **Accuaracy**"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

LR = LogisticRegression(random_state=42)
DT = DecisionTreeClassifier(random_state=42)

models = {
    'Logistic Regression': LR,
    'SVM': svc,
    'Decision Tree': DT
}
accuracies = {}

for model_name, model in models.items():
    model.fit(X_train, y_train)  # Fit the model
    y_test_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_test_pred)
    accuracies[model_name] = accuracy

plt.figure(figsize=(10, 6))
sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette="viridis")

for i, (model_name, accuracy) in enumerate(accuracies.items()):
    plt.text(i, accuracy + 0.01, f"{accuracy:.2f}", ha='center', va='bottom', fontsize=10)

plt.title("Model Accuracy Comparison")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.show()